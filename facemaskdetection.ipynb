{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lit-cup/finalterm_facemaskdetection/blob/main/facemaskdetection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkG0EOoTMX8t"
      },
      "source": [
        "# **Prepare code**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z6ez0Ya-MKs9"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/lit-cup/finalterm_facemaskdetection.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrtJREPD7IDK"
      },
      "source": [
        "# **Read Kaggle Token**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RC6dUUYdt1Wy"
      },
      "outputs": [],
      "source": [
        "! pip install -q kaggle\n",
        "from google.colab import files\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UBc_UkK7Vce_"
      },
      "outputs": [],
      "source": [
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle\n",
        "! chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0shHQ13Z7OSI"
      },
      "source": [
        "# **Create input put datasets files**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h21vjpY8tFKB"
      },
      "outputs": [],
      "source": [
        "! mkdir finalterm_facemaskdetection/input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hir5IdI5wkBh",
        "outputId": "c01272ba-e2cb-4bbc-c104-31e9592e4a84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading face-mask-detection.zip to /content\n",
            "100% 397M/398M [00:18<00:00, 17.6MB/s]\n",
            "100% 398M/398M [00:18<00:00, 22.3MB/s]\n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download -d andrewmvd/face-mask-detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxhcLQoa6jiy"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "! unzip face-mask-detection.zip -d finalterm_facemaskdetection/input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGHjcH0TTK-c"
      },
      "source": [
        "# **Import Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5CH-GCqKMybv"
      },
      "outputs": [],
      "source": [
        "# Nedded Libraries\n",
        "\n",
        "# PyTorch\n",
        "import torch \n",
        "import torchvision\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import models\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor, fasterrcnn_resnet50_fpn\n",
        "import albumentations as A\n",
        "\n",
        "# Image processing\n",
        "from PIL import Image, ImageDraw, ExifTags, ImageColor, ImageFont\n",
        "\n",
        "# Image Plots\n",
        "from matplotlib import pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "# Data managements\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# File interpretation\n",
        "import os\n",
        "import xml.etree.ElementTree as ET\n",
        "import random\n",
        "\n",
        "# Others\n",
        "import time\n",
        "from collections import Counter\n",
        "from random import seed, randint\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpEYKfIXTems"
      },
      "source": [
        "# **Find Annotation Files**\n",
        "Indicate the path for the annotation files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bWxui26LTgH9"
      },
      "outputs": [],
      "source": [
        "# Annotations directory path\n",
        "ann_directory = '/content/finalterm_facemaskdetection/input/annotations'\n",
        "\n",
        "# List directory\n",
        "ann_files = os.listdir(ann_directory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBUefTOu70R4"
      },
      "source": [
        "# **Find Image Files**\n",
        "Indicate the path for the image files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v37B9Iy871hY"
      },
      "outputs": [],
      "source": [
        "# Image directory path\n",
        "img_directory = '/content/finalterm_facemaskdetection/input/images'\n",
        "\n",
        "# List directory\n",
        "img_files = os.listdir(img_directory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ICXc_jF-jmU"
      },
      "source": [
        "# **Helper Functions**\n",
        "This are auxiliary functions used throughout the notebook. This way the notebook stays tidy and clean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zioBpNp58S-x"
      },
      "outputs": [],
      "source": [
        "def draw_bounding_boxes(img_tensor, target=None, prediction=None):\n",
        "    \"\"\"Draws bounding boxes in given images. Displays them\n",
        "\n",
        "        Inputs:\n",
        "          img:\n",
        "            Image in tensor format.\n",
        "          target:\n",
        "            target dictionary containing bboxes list wit format -> [xmin, ymin, xmax, ymax]\n",
        "\n",
        "        Returns:\n",
        "          None\n",
        "        \"\"\"\n",
        "\n",
        "    img = torchvision.transforms.ToPILImage()(img_tensor)\n",
        "\n",
        "    # fetching the dimensions\n",
        "    wid, hgt = img.size\n",
        "    print(str(wid) + \"x\" + str(hgt))\n",
        "\n",
        "    # Img to draw in\n",
        "    draw = ImageDraw.Draw(img)\n",
        "\n",
        "    if target:\n",
        "        target_bboxes = target['boxes'].numpy().tolist()\n",
        "        target_labels = decode_labels(target['labels'].numpy())\n",
        "\n",
        "        for i in range(len(target_bboxes)):\n",
        "            # Create Rectangle patches and add the patches to the axes\n",
        "            draw.rectangle(target_bboxes[i], fill=None, outline='green', width=2)\n",
        "            draw.text(target_bboxes[i][:2], target_labels[i], fill='green', font=None, anchor=None, spacing=4,\n",
        "                      align='left', direction=None, features=None, language=None, stroke_width=0, stroke_fill=None,\n",
        "                      embedded_color=False)\n",
        "\n",
        "    if prediction:\n",
        "        prediction_bboxes = prediction['boxes'].detach().cpu().numpy().tolist()\n",
        "        prediction_labels = decode_labels(prediction['labels'].detach().cpu().numpy())\n",
        "        for i in range(len(prediction_bboxes)):\n",
        "            # Create Rectangle patches and add the patches to the axes\n",
        "            draw.rectangle(prediction_bboxes[i], fill=None, outline='red', width=2)\n",
        "            draw.text(prediction_bboxes[i][:2], prediction_labels[i], fill='red', font=None, anchor=None, spacing=4,\n",
        "                      align='left', direction=None, features=None, language=None, stroke_width=0, stroke_fill=None,\n",
        "                      embedded_color=False)\n",
        "\n",
        "    display(img)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ifYn9VS8X4p"
      },
      "outputs": [],
      "source": [
        "def encoded_labels(lst_labels):\n",
        "    \"\"\"Encodes label classes from string to integers.\n",
        "\n",
        "        Labels are encoded accordingly:\n",
        "            - background => 0\n",
        "            - with_mask => 1\n",
        "            - mask_weared_incorrect => 2\n",
        "            - without_mask => 3\n",
        "\n",
        "            Args:\n",
        "              lst_labels:\n",
        "                A list with classes in string format (e.g. ['with_mask', 'mask_weared_incorrect'...]).\n",
        "\n",
        "            Returns:\n",
        "              encoded:\n",
        "                A list with integers that represent each class.\n",
        "            \"\"\"\n",
        "\n",
        "    encoded=[]\n",
        "    for label in lst_labels:\n",
        "        if label == \"with_mask\":\n",
        "            code = 1\n",
        "        elif label == \"mask_weared_incorrect\":\n",
        "            code = 2\n",
        "        elif label == \"without_mask\":\n",
        "            code = 3\n",
        "        else:\n",
        "            code = 0\n",
        "        encoded.append(code)\n",
        "    return encoded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tq5wHk688dZh"
      },
      "outputs": [],
      "source": [
        "def decode_labels(lst_labels):\n",
        "    \"\"\"\n",
        "    Decode label classes from integers to strings.\n",
        "    Labels are encoded accordingly:\n",
        "        - background => 0\n",
        "        - with_mask => 1\n",
        "        - mask_weared_incorrect => 2\n",
        "        - without_mask => 3\n",
        "\n",
        "    Args:\n",
        "      lst_labels:\n",
        "        A list with classes in integer format (e.g. [1, 2, ...]).\n",
        "\n",
        "    Returns:\n",
        "        A list with strings that represent each class.\n",
        "    \"\"\"\n",
        "\n",
        "    labels=[]\n",
        "    for code in lst_labels:\n",
        "        if code == 1:\n",
        "            label = \"with_mask\"\n",
        "        elif code == 2:\n",
        "            label = \"mask_weared_incorrect\"\n",
        "        elif code == 3:\n",
        "            label = \"without_mask\"\n",
        "        else:\n",
        "            label = 'background'\n",
        "        labels.append(label)\n",
        "    return labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4F-8WloK8fVg"
      },
      "outputs": [],
      "source": [
        "def build_model(nclasses):\n",
        "    \"\"\"\n",
        "    Builds model. Uses Faster R-CNN pre-trained on COCO dataset.\n",
        "\n",
        "    Args:\n",
        "      nclasses:\n",
        "        number of classes\n",
        "\n",
        "    Return:\n",
        "      model: Faster R-CNN pre-trained model\n",
        "    \"\"\"\n",
        "    # load pre-trained model on COCO\n",
        "    model = fasterrcnn_resnet50_fpn(pretrained=True, min_size=400, max_size=700)\n",
        "\n",
        "    # get the number of input features for the classifier\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "\n",
        "    # replace the pre-trained head with a new one\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, nclasses)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vb7p57Tx8iZ4"
      },
      "outputs": [],
      "source": [
        "def train_model(model, loader, optimizer, scheduler, epochs, device):\n",
        "  \"\"\" \n",
        "    Inputs:\n",
        "      - model\n",
        "      - loader: Dataloader PyTorch object with training data\n",
        "      - optimizer\n",
        "      - scheduler\n",
        "      - epochs\n",
        "      - device\n",
        "\n",
        "    Returns:\n",
        "      - model\n",
        "      - loss_list: list with mean loss per epoch. Epoch 1 is in idex 0.\n",
        "    \"\"\"\n",
        "  # Create a loss list to keep epoch average loss\n",
        "  loss_list = []\n",
        "  # Epochs\n",
        "  for epoch in range(epochs):\n",
        "      print('Starting epoch...... {}/{} '.format(epoch + 1, epochs))\n",
        "      iteration = 0\n",
        "      loss_sub_list = []\n",
        "      start = time.time()\n",
        "      for images, targets in loader:\n",
        "          # Agregate images in batch loader\n",
        "          images = list(image.to(device) for image in images)\n",
        "\n",
        "          # Agregate targets in batch loader\n",
        "          targets = [{key: val.to(device) for key, val in target.items()} for target in targets]\n",
        "\n",
        "          # Sets model to train mode (just a flag)\n",
        "          model.train()\n",
        "\n",
        "          # Output of model returns loss and detections\n",
        "          optimizer.zero_grad()\n",
        "          output = model(images, targets)\n",
        "\n",
        "          # Calculate Cost\n",
        "          losses = sum(loss for loss in output.values())\n",
        "          loss_value = losses.item()\n",
        "          loss_sub_list.append(loss_value)\n",
        "          print('')\n",
        "\n",
        "          # Update optimizer and learning rate\n",
        "          losses.backward()\n",
        "          optimizer.step()\n",
        "          iteration += 1\n",
        "          print('Iteration: {:d} --> Loss: {:.3f}'.format(iteration, loss_value))\n",
        "          \n",
        "      end = time.time()\n",
        "      # update scheduler\n",
        "      scheduler.step()\n",
        "      # print the loss of epoch\n",
        "      epoch_loss = np.mean(loss_sub_list)\n",
        "      loss_list.append(epoch_loss)\n",
        "      print('Epoch loss: {:.3f} , time used: ({:.1f}s)'.format(epoch_loss, end - start))\n",
        "      \n",
        "  return model, loss_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3548hAyG8ptw"
      },
      "outputs": [],
      "source": [
        "def apply_nms(orig_prediction, iou_thresh):\n",
        "    \"\"\"\n",
        "    Applies non max supression and eliminates low score bounding boxes.\n",
        "\n",
        "      Args:\n",
        "        orig_prediction: the model output. A dictionary containing element scores and boxes.\n",
        "        iou_thresh: Intersection over Union threshold. Every bbox prediction with an IoU greater than this value\n",
        "                      gets deleted in NMS.\n",
        "\n",
        "      Returns:\n",
        "        final_prediction: Resulting prediction\n",
        "    \"\"\"\n",
        "\n",
        "    # torchvision returns the indices of the bboxes to keep\n",
        "    keep = torchvision.ops.nms(orig_prediction['boxes'], orig_prediction['scores'], iou_thresh)\n",
        "\n",
        "    # Keep indices from nms\n",
        "    final_prediction = orig_prediction\n",
        "    final_prediction['boxes'] = final_prediction['boxes'][keep]\n",
        "    final_prediction['scores'] = final_prediction['scores'][keep]\n",
        "    final_prediction['labels'] = final_prediction['labels'][keep]\n",
        "\n",
        "    return final_prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ZB_s3Lm8r7w"
      },
      "outputs": [],
      "source": [
        "def remove_low_score_bb(orig_prediction, score_thresh):\n",
        "    \"\"\"\n",
        "    Eliminates low score bounding boxes.\n",
        "\n",
        "    Args:\n",
        "        orig_prediction: the model output. A dictionary containing element scores and boxes.\n",
        "        score_thresh: Boxes with a lower confidence score than this value get deleted\n",
        "\n",
        "    Returns:\n",
        "        final_prediction: Resulting prediction\n",
        "    \"\"\"\n",
        "\n",
        "    # Remove low confidence scores according to given threshold\n",
        "    index_list_scores = []\n",
        "    scores = orig_prediction['scores'].detach().cpu().numpy()\n",
        "    for i in range(len(scores)):\n",
        "        if scores[i] > score_thresh:\n",
        "            index_list_scores.append(i)\n",
        "    keep = torch.tensor(index_list_scores)\n",
        "\n",
        "    # Keep indices from high score bb\n",
        "    final_prediction = orig_prediction\n",
        "    final_prediction['boxes'] = final_prediction['boxes'][keep]\n",
        "    final_prediction['scores'] = final_prediction['scores'][keep]\n",
        "    final_prediction['labels'] = final_prediction['labels'][keep]\n",
        "\n",
        "    return final_prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0wdhbNj8trQ"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "    # Collate function for Dataloader\n",
        "    return tuple(zip(*batch))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8yi2SlO98v-Y"
      },
      "outputs": [],
      "source": [
        "def IOU(box1, box2):\n",
        "    '''\n",
        "    Intersection over Union - IoU\n",
        "    *------------\n",
        "    |   (x2min,y2min)\n",
        "    |   *----------\n",
        "    |   | ######| |\n",
        "    ----|------* (x1max,y1max)\n",
        "        |         |\n",
        "        ----------\n",
        "\n",
        "    Args:\n",
        "        box1: [xmin,ymin,xmax,ymax]\n",
        "        box2: [xmin,ymin,xmax,ymax]\n",
        "\n",
        "    Returns:\n",
        "        iou -> value of intersection over union of the 2 boxes\n",
        "\n",
        "    '''\n",
        "\n",
        "    # Compute coordinates of intersection\n",
        "    xmin_inter = max(box1[0], box2[0])\n",
        "    ymin_inter = max(box1[1], box2[1])\n",
        "    xmax_inter = min(box1[2], box2[2])\n",
        "    ymax_inter = min(box1[3], box2[3])\n",
        "\n",
        "    # calculate area of intersection rectangle\n",
        "    inter_area = max(0, xmax_inter - xmin_inter + 1) * max(0, ymax_inter - ymin_inter + 1) # FIXME why plus one?\n",
        " \n",
        "    # calculate boxes areas\n",
        "    area1 = (box1[2] - box1[0] + 1) * (box1[3] - box1[1] + 1)\n",
        "    area2 = (box2[2] - box2[0] + 1) * (box2[3] - box2[1] + 1)\n",
        " \n",
        "    # compute IoU\n",
        "    iou = inter_area / float(area1 + area2 - inter_area)\n",
        "    assert iou >= 0\n",
        "    return iou\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5jgetvqu8yrY"
      },
      "outputs": [],
      "source": [
        "def compute_AP(ground_truth, predictions, iou_thresh=0.5, n_classes=4):\n",
        "    \"\"\"\n",
        "    Calculates Average Precision across all classes.\n",
        "\n",
        "    Args:\n",
        "        ground_truth: list with ground-truth objects. Needs to have the following format: [sequence, frame, obj, [xmin, ymin, xmax, ymax], label, score]\n",
        "        predictions: list with predictions objects. Needs to have the following format: [sequence, frame, obj, [xmin, ymin, xmax, ymax], label, score]\n",
        "        iou_thresh: IoU to which a prediction compared to a ground-truth is considered right.\n",
        "        n_classes: number of existent classes\n",
        "\n",
        "    Returns:\n",
        "        Average precision for the specified threshold.\n",
        "    \"\"\"\n",
        "    # Initialize lists\n",
        "    APs = []\n",
        "    class_gt = []\n",
        "    class_predictions = []\n",
        "\n",
        "    # AP is computed for each class\n",
        "    for c in range(n_classes):\n",
        "        # Find gt and predictions of the class\n",
        "        for gt in ground_truth:\n",
        "            if gt[4] == c:\n",
        "                class_gt.append(gt)\n",
        "        for predict in predictions:\n",
        "            if predict[4] == c:\n",
        "                class_predictions.append(predict)\n",
        "\n",
        "        # Create dict with array of zeros for bb in each image\n",
        "        gt_amount_bb = Counter([gt[1] for gt in class_gt])\n",
        "        for key, val in gt_amount_bb.items():\n",
        "            gt_amount_bb[key] = np.zeros(val)\n",
        "\n",
        "        # Sort class predictions by their score\n",
        "        class_predictions = sorted(class_predictions, key=lambda x: x[5], reverse=True)\n",
        "\n",
        "        # Create arrays for Positives (True and False)\n",
        "        TP = np.zeros(len(class_predictions))\n",
        "        FP = np.zeros(len(class_predictions))\n",
        "        # Number of true boxes\n",
        "        truth = len(class_gt)\n",
        "\n",
        "        # Initializing aux variables\n",
        "        epsilon = 1e-6\n",
        "\n",
        "        # Iterate over predictions in each image and compare with ground truth\n",
        "        for predict_idx, prediction in enumerate(class_predictions):\n",
        "            # Filter prediction image ground truths\n",
        "            image_gt = [obj for obj in class_gt if obj[1] == prediction[1]]\n",
        "\n",
        "            # Initializing aux variables\n",
        "            best_iou = -1\n",
        "            best_gt_iou_idx = -1\n",
        "\n",
        "            # Iterate through image ground truths and calculate IoUs\n",
        "            for gt_idx, gt in enumerate(image_gt):\n",
        "                iou = IOU(prediction[3], gt[3])\n",
        "                if iou > best_iou:\n",
        "                    best_iou = iou\n",
        "                    best_gt_iou_idx = gt_idx\n",
        "\n",
        "            # If the best IoU is greater that thresh than an TP prediction has been found\n",
        "            if best_iou > iou_thresh and best_gt_iou_idx > -1:\n",
        "                # Check if gt box was already covered\n",
        "                if  gt_amount_bb[prediction[1]][best_gt_iou_idx] == 0:\n",
        "                    gt_amount_bb[prediction[1]][best_gt_iou_idx] = 1  # set as covered\n",
        "                    TP[predict_idx] = 1  # Count as true positive\n",
        "                else:\n",
        "                    FP[predict_idx] = 1\n",
        "            else:\n",
        "                FP[predict_idx] = 1\n",
        "\n",
        "        # Calculate recall and precision\n",
        "        TP_cumsum = np.cumsum(TP)\n",
        "        FP_cumsum = np.cumsum(FP)\n",
        "        recall = np.append([0], TP_cumsum / (truth + epsilon))\n",
        "        precision = np.append([1], np.divide(TP_cumsum, (TP_cumsum + FP_cumsum + epsilon)))\n",
        "\n",
        "        # Calculate the area precision/recall and add to list\n",
        "        APs.append(np.trapz(precision, recall))\n",
        "\n",
        "    return sum(APs)/len(APs)  # average of class precisions\n",
        "\n",
        "\n",
        "def compute_mAP(ground_truth, predictions, n_classes):\n",
        "    \"\"\"\n",
        "    Calls AP computation for different levels of IoUs, [0.5:.05:0.95].\n",
        "\n",
        "    Args:\n",
        "        ground_truth: list with ground-truth objects. Needs to have the following format: [sequence, frame, obj, [xmin, ymin, xmax, ymax], label, score]\n",
        "        predictions: list with predictions objects. Needs to have the following format: [sequence, frame, obj, [xmin, ymin, xmax, ymax], label, score]\n",
        "        n_classes: number of existent classes.\n",
        "\n",
        "    Returns:\n",
        "        mAp and list with APs for each IoU threshold.\n",
        "    \"\"\"\n",
        "    # return mAP\n",
        "    APs = [compute_AP(ground_truth, predictions, iou_thresh, n_classes) for iou_thresh in np.arange(0.5, 1.0, 0.05)]\n",
        "    return np.mean(APs), APs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H8X259E082Vw"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def evaluate(model, data_loader, device, sequences=1):\n",
        "    \"\"\"\n",
        "    Evaluates model mAP for IoU range of [0.5:.05:0.95].\n",
        "\n",
        "    Args:\n",
        "        model: -\n",
        "        data_loader: -\n",
        "        device: -\n",
        "        sequences: the number of sequences of images to pass, if any\n",
        "\n",
        "    Returns:\n",
        "        mAP and AP list for each IoU threshold in range [0.5:.05:0.95]\n",
        "    \"\"\"\n",
        "\n",
        "    # Set evaluation mode flag\n",
        "    model.eval()\n",
        "    # Create list with all object detection -> [set, frame, obj, [xmin,ymin,xmax,ymax], label, score]\n",
        "    ground_truth = []\n",
        "    predictions = []\n",
        "\n",
        "    # Gather all targets and outputs on test set\n",
        "    for image, targets in data_loader:\n",
        "        image = [img.to(device) for img in image]\n",
        "        outputs = model(image)\n",
        "        for idx in range(len(outputs)):\n",
        "            outputs[idx] = apply_nms(outputs[idx], iou_thresh=0.5)\n",
        "\n",
        "        # create list for targets and outputs to pass to compute_mAP()\n",
        "        # lists have the following structure:  [sequence, frame, obj_idx, [xmin, ymin, xmax, ymax], label, score]\n",
        "        for s in range(sequences):\n",
        "            obj_gt = 0\n",
        "            obj_target = 0\n",
        "            for out, target in zip(outputs, targets):\n",
        "\n",
        "                for i in range(len(target['boxes'])):\n",
        "                    ground_truth.append([s, target['image_id'].detach().cpu().numpy()[0], obj_target,\n",
        "                                         target['boxes'].detach().cpu().numpy()[i],\n",
        "                                         target['labels'].detach().cpu().numpy()[i], 1])\n",
        "                    obj_target += 1\n",
        "\n",
        "                for j in range(len(out['boxes'])):\n",
        "                    predictions.append([s, target['image_id'].detach().cpu().numpy()[0], obj_gt,\n",
        "                                        out['boxes'].detach().cpu().numpy()[j],\n",
        "                                        out['labels'].detach().cpu().numpy()[j],\n",
        "                                        out['scores'].detach().cpu().numpy()[j]])\n",
        "                    obj_gt += 1\n",
        "\n",
        "    mAP, AP = compute_mAP(ground_truth, predictions, n_classes=4)\n",
        "    print(\"mAP:{:.3f}\".format(mAP))\n",
        "    for ap_metric, iou in zip(AP, np.arange(0.5, 1, 0.05)):\n",
        "        print(\"\\tAP at IoU level [{:.2f}]: {:.3f}\".format(iou, ap_metric))\n",
        "\n",
        "    return mAP, AP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Y25g21M9FF6"
      },
      "source": [
        "# **Create Dataset Class**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CrvztzGT855o"
      },
      "outputs": [],
      "source": [
        "# Create dataset object\n",
        "class MyDataset(Dataset):\n",
        "\n",
        "    # Constructor\n",
        "    def __init__(self, ann_dir, img_dir, transform=None, mode='train'):\n",
        "\n",
        "        # Image directories\n",
        "        self.ann_dir = ann_dir\n",
        "        self.img_dir = img_dir\n",
        "\n",
        "        # The transform is goint to be used on image\n",
        "        self.transform = transform\n",
        "\n",
        "        # Create dataframe to hold info\n",
        "        self.data = pd.DataFrame(columns=['Filename', 'BoundingBoxes', 'Labels', 'Area', 'N_Objects'])\n",
        "\n",
        "        # Append rows with image filename and respective bounding boxes to the df\n",
        "        for file in enumerate(os.listdir(img_dir)):\n",
        "\n",
        "            # Find image annotation file\n",
        "            ann_file_path = os.path.join(ann_dir, file[1][:-4]) + '.xml'\n",
        "\n",
        "            # Read XML file and return bounding boxes and class attributes\n",
        "            objects = self.read_XML_classf(ann_file_path)\n",
        "\n",
        "            # Create list of labels in an image\n",
        "            list_labels = encoded_labels(objects[0]['labels'])\n",
        "\n",
        "            # Create list of bounding boxes in an image\n",
        "            list_bb = []\n",
        "            list_area = []\n",
        "            n_obj = len(objects[0]['objects'])\n",
        "            for i in objects[0]['objects']:\n",
        "                list = [i['xmin'], i['ymin'], i['xmax'], i['ymax']]\n",
        "                list_bb.append(list)\n",
        "                list_area.append((i['xmax'] - i['xmin']) * (i['ymax'] - i['ymin']))\n",
        "\n",
        "            # Create dataframe object with row containing [(Image file name),(Bounding Box List)]\n",
        "            df = pd.DataFrame([[file[1], list_bb, list_labels, list_area, n_obj]],\n",
        "                              columns=['Filename', 'BoundingBoxes', 'Labels', 'Area', 'N_Objects'])\n",
        "            self.data = self.data.append(df)\n",
        "\n",
        "        if mode == 'train':\n",
        "            self.data = self.data[:680]\n",
        "        elif mode == 'validation':\n",
        "            self.data = self.data[680:700]\n",
        "        elif mode == 'test':\n",
        "            self.data = self.data[700:850]\n",
        "\n",
        "        # Number of images in dataset\n",
        "        self.len = self.data.shape[0]\n",
        "\n",
        "        # Get the length\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "    # Getter\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        # Image file path\n",
        "        img_name = os.path.join(self.img_dir, self.data.iloc[idx, 0])\n",
        "\n",
        "        # Open image file and tranform to tensor\n",
        "        img = Image.open(img_name).convert('RGB')\n",
        "\n",
        "        # Get bounding box coordinates\n",
        "        bbox = torch.tensor(self.data.iloc[idx, 1])\n",
        "\n",
        "        # Get labels\n",
        "        labels = torch.tensor(self.data.iloc[idx, 2])\n",
        "\n",
        "        # Get bounding box areas\n",
        "        area = torch.tensor(self.data.iloc[idx, 3])\n",
        "\n",
        "        # If any, aplly tranformations to image and bounding box mask\n",
        "        if self.transform:\n",
        "            # Convert PIL image to numpy array\n",
        "            img = np.array(img)\n",
        "            # Apply transformations\n",
        "            transformed = self.transform(image=img, bboxes=bbox)\n",
        "            # Convert numpy array to PIL Image\n",
        "            img = Image.fromarray(transformed['image'])\n",
        "            # Get transformed bb\n",
        "            bbox = torch.tensor(transformed['bboxes'])\n",
        "\n",
        "        # suppose all instances are not crowd\n",
        "        num_objs = self.data.iloc[idx, 4]\n",
        "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
        "\n",
        "        # Transform img to tensor\n",
        "        img = torchvision.transforms.ToTensor()(img)\n",
        "\n",
        "        # Build Targer dict\n",
        "        target= {\"boxes\": bbox, \"labels\": labels, \"image_id\": torch.tensor([idx]), \"area\": area, \"iscrowd\": iscrowd}\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    # XML reader -> returns dictionary with image bounding boxes sizes\n",
        "    def read_XML_classf(self, ann_file_path):\n",
        "        bboxes = [{\n",
        "            'file': ann_file_path,\n",
        "            'labels': [],\n",
        "            'objects': []\n",
        "        }]\n",
        "\n",
        "        # Reading XML file objects and print Bounding Boxes\n",
        "        tree = ET.parse(ann_file_path)\n",
        "        root = tree.getroot()\n",
        "        objects = root.findall('object')\n",
        "\n",
        "        for obj in objects:\n",
        "            # label\n",
        "            label = obj.find('name').text\n",
        "            bboxes[0]['labels'].append(label)\n",
        "\n",
        "            # bbox dimensions\n",
        "            bndbox = obj.find('bndbox')\n",
        "            xmin = int(bndbox.find('xmin').text)\n",
        "            ymin = int(bndbox.find('ymin').text)\n",
        "            xmax = int(bndbox.find('xmax').text)\n",
        "            ymax = int(bndbox.find('ymax').text)\n",
        "            bboxes[0]['objects'].append({'xmin': xmin, 'ymin': ymin, 'xmax': xmax, 'ymax': ymax})\n",
        "\n",
        "        return bboxes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40R_wv7p8--4"
      },
      "source": [
        "# **Create Data Pipeline**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YVptAzpj893Y"
      },
      "outputs": [],
      "source": [
        "# Create Data Pipeline\n",
        "\n",
        "# Training Data\n",
        "dataset_train = MyDataset(ann_directory,img_directory, mode = 'train')\n",
        "loader_train = DataLoader(dataset_train, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
        "# Validation Data\n",
        "dataset_validation = MyDataset(ann_directory,img_directory, mode = 'validation')\n",
        "loader_val = DataLoader(dataset_validation, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
        "# Test Data\n",
        "dataset_test = MyDataset(ann_directory,img_directory, mode = 'test')\n",
        "loader_test = DataLoader(dataset_test, batch_size=4, shuffle=True, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mdbhJpUI9NmA"
      },
      "outputs": [],
      "source": [
        "# pick one image from the train set\n",
        "img, target = dataset_train[0]\n",
        "draw_bounding_boxes(img, target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KayJ-RG9ToY"
      },
      "source": [
        "# **Setting up the Faster R-CNN Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EMsklQj_9S1I"
      },
      "outputs": [],
      "source": [
        "# Setting up GPU device\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "# Nº of classes: background, with_mask, mask_weared_incorrect, without_mask and build model (faster r-cnn)\n",
        "num_classes = 4 \n",
        "model = build_model(num_classes)\n",
        "\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fvG34H0c9YQa"
      },
      "outputs": [],
      "source": [
        "# Set Hyper-parameters\n",
        "\n",
        "# Network params\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "\n",
        "# Optimizers\n",
        "optimizer = torch.optim.Adam(params, lr=0.0001)\n",
        "#optimizer = torch.optim.SGD(params, lr=0.005)\n",
        "\n",
        "# Learning Rate, lr decreases to half every 2 epochs \n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.5)\n",
        "\n",
        "# Number of epochs to perform\n",
        "epochs=20"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xKMMRV59dIY"
      },
      "source": [
        "# **Train the model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNdwmASX9eJg"
      },
      "outputs": [],
      "source": [
        "pip install git+https://github.com/gautamchitnis/cocoapi.git@cocodataset-master#subdirectory=PythonAPI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PGMqb5AD9jyg"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/pytorch/vision.git\n",
        "%cd vision\n",
        "!git checkout v0.3.0\n",
        "\n",
        "!cp references/detection/utils.py ../\n",
        "!cp references/detection/transforms.py ../\n",
        "!cp references/detection/coco_eval.py ../\n",
        "!cp references/detection/engine.py ../\n",
        "!cp references/detection/coco_utils.py ../\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Hc8K4t8w9myf"
      },
      "outputs": [],
      "source": [
        "from engine import train_one_epoch\n",
        "# Training\n",
        "for epoch in range(epochs):\n",
        "    # train for one epoch, printing every 50 iterations\n",
        "    train_one_epoch(model, optimizer, loader_train, device, epoch, print_freq=20)\n",
        "    # update the learning rate\n",
        "    lr_scheduler.step()\n",
        "    # evaluate on the test dataset\n",
        "    evaluate(model, loader_val, device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfnS5fbI913v"
      },
      "source": [
        "# **Saving the Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NV24hPIL93ur"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "# Save model with current date\n",
        "now = datetime.now()\n",
        "d = now.strftime(\"%Y_%b_%d_%Hh_%mm\")\n",
        "PATH = 'model_'+d+'.pt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rd1ncU_Z97Io"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0nA52riv98an"
      },
      "outputs": [],
      "source": [
        "# Get saved model\n",
        "model_eval = model.load_state_dict(torch.load(PATH))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TTWkhrA099uY"
      },
      "outputs": [],
      "source": [
        "# put the model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Evaluate the model\n",
        "evaluate(model, loader_test, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s5oLloyO99oY"
      },
      "outputs": [],
      "source": [
        "# Make prediction on random image\n",
        "n = randint(0, dataset_test.len)\n",
        "img, target = dataset_test[n]\n",
        "with torch.no_grad():\n",
        "    prediction = model([img.to(device)])[0]\n",
        "\n",
        "# Non max suppression to reduce the number of bounding boxes\n",
        "nms_prediction = apply_nms(prediction, iou_thresh=0.5)\n",
        "# Remove low score boxes below score_thresh\n",
        "filtered_prediction = remove_low_score_bb(nms_prediction, score_thresh=0.3)\n",
        "\n",
        "# Draw bounding boxes\n",
        "draw_bounding_boxes(img.detach().cpu(), target=target, prediction=filtered_prediction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CYTJHn7a-Bro"
      },
      "outputs": [],
      "source": [
        "from engine import evaluate as eval \n",
        "eval(model, loader_test, device=device)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOm5n+FD19mXk3tnL4Updo3",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}